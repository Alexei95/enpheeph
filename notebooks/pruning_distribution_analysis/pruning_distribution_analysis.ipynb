# enpheeph - Neural Fault Injection Framework
# Copyright (C) 2020-2023 Alessio "Alexei95" Colucci
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437a3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import time\n",
    "\n",
    "import captum\n",
    "import numpy\n",
    "import pandas\n",
    "import plotly\n",
    "import plotly.express\n",
    "import torch\n",
    "import torch.optim\n",
    "import torchinfo\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbfb1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_layer_name_from_summary(layer_summary, skip_main_model=True):\n",
    "    parent_info = layer_summary.parent_info\n",
    "    layer_full_name = layer_summary.var_name\n",
    "    while parent_info is not None and (not skip_main_model or skip_main_model and parent_info.parent_info is not None):\n",
    "        layer_full_name = f\"{parent_info.var_name}.{layer_full_name}\"\n",
    "        parent_info = parent_info.parent_info\n",
    "    return layer_full_name\n",
    "\n",
    "def get_layer_from_full_name(model, layer_name, separator=\".\", main_model_is_in_the_layer_name=False):\n",
    "    module = model\n",
    "    if main_model_is_in_the_layer_name:\n",
    "        layer_name = separator.join(layer_name.split(separator)[1:])\n",
    "    for l_n in layer_name.split(separator):\n",
    "        module = getattr(module, l_n)\n",
    "    return module\n",
    "\n",
    "def get_attributions(model, dataloader, layer_name_list, attributions_checkpoint_path, attribution=captum.attr.LayerConductance, save_checkpoint=True, load_checkpoint=True):\n",
    "    if attributions_checkpoint_path.exists() and load_checkpoint:\n",
    "        attributions = torch.load(str(attributions_checkpoint_path))\n",
    "        return attributions\n",
    "    elif save_checkpoint:\n",
    "        attributions_checkpoint_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "    model = model.train(False).to(torch.device(\"cpu\"))\n",
    "#     input_list, target_list = [], []\n",
    "    \n",
    "#         input_list.append(x.to(torch.device(\"cpu\")))\n",
    "#         target_list.append(y.to(torch.device(\"cpu\")))\n",
    "#         if n_element_threshold is not None and (idx + 1) * dataloader.batch_size >= n_element_threshold:\n",
    "#             break\n",
    "#     input_ = torch.cat(input_list, dim=0)\n",
    "#     target = torch.cat(target_list, dim=0)\n",
    "#     print(input_.size())\n",
    "    \n",
    "    attributions = {}\n",
    "    for layer_name in layer_name_list:\n",
    "        print(layer_name)\n",
    "        layer_attributions = []\n",
    "        attr_instance = attribution(model, get_layer_from_full_name(model, layer_name))\n",
    "        for idx, b in enumerate(dataloader):\n",
    "            x, y = b\n",
    "            attr, delta = attr_instance.attribute(\n",
    "                inputs=x.to(torch.device(\"cpu\")),\n",
    "                target=y.to(torch.device(\"cpu\")),\n",
    "                return_convergence_delta=True,\n",
    "            )\n",
    "            layer_attributions.append(\n",
    "                [attr.detach(), delta.detach(), ],\n",
    "            )\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"Batches done: {idx}\")\n",
    "#                 break\n",
    "        attributions[layer_name] = layer_attributions\n",
    "    \n",
    "    if save_checkpoint:\n",
    "        torch.save(attributions, str(attributions_checkpoint_path))\n",
    "    \n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0213829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIME_FORMAT = \"%Y_%m_%d__%H_%M_%S_%z\"\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    random.seed(seed)\n",
    "    numpy.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "    return seed\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device(\"cuda:2\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40dabae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary = torchinfo.summary(model, input_size=[1, 3, 32, 32])\n",
    "# summary.summary_list[2].output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e1be9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL_LAYER_LIST = {}\n",
    "# for layer_summary in filter(lambda x: x.is_leaf_layer and x.output_size is not None, torchinfo.summary(model, input_size=[1, 3, 32, 32]).summary_list):\n",
    "#     layer_full_name = get_full_layer_name_from_summary(layer_summary, skip_main_model=True)\n",
    "#     FULL_LAYER_LIST[layer_full_name] = layer_summary.output_size[1:]\n",
    "# FULL_LAYER_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b03ca8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features.0': [64, 32, 32],\n",
       " 'features.1': [64, 32, 32],\n",
       " 'features.2': [64, 16, 16],\n",
       " 'features.3': [128, 16, 16],\n",
       " 'features.4': [128, 16, 16],\n",
       " 'features.5': [128, 8, 8],\n",
       " 'features.6': [256, 8, 8],\n",
       " 'features.7': [256, 8, 8],\n",
       " 'features.8': [256, 8, 8],\n",
       " 'features.9': [256, 8, 8],\n",
       " 'features.10': [256, 4, 4],\n",
       " 'features.11': [512, 4, 4],\n",
       " 'features.12': [512, 4, 4],\n",
       " 'features.13': [512, 4, 4],\n",
       " 'features.14': [512, 4, 4],\n",
       " 'features.15': [512, 2, 2],\n",
       " 'features.16': [512, 2, 2],\n",
       " 'features.17': [512, 2, 2],\n",
       " 'features.18': [512, 2, 2],\n",
       " 'features.19': [512, 2, 2],\n",
       " 'features.20': [512, 1, 1],\n",
       " 'avgpool': [512, 7, 7],\n",
       " 'classifier.0': [4096],\n",
       " 'classifier.1': [4096],\n",
       " 'classifier.2': [4096],\n",
       " 'classifier.3': [4096],\n",
       " 'classifier.4': [4096],\n",
       " 'classifier.5': [4096],\n",
       " 'classifier.6': [10]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VGG11_CIFAR10_FULL_LAYER_LIST = {\n",
    "    'features.0': [64, 32, 32],\n",
    "    'features.1': [64, 32, 32],\n",
    "    'features.2': [64, 16, 16],\n",
    "    'features.3': [128, 16, 16],\n",
    "    'features.4': [128, 16, 16],\n",
    "    'features.5': [128, 8, 8],\n",
    "    'features.6': [256, 8, 8],\n",
    "    'features.7': [256, 8, 8],\n",
    "    'features.8': [256, 8, 8],\n",
    "    'features.9': [256, 8, 8],\n",
    "    'features.10': [256, 4, 4],\n",
    "    'features.11': [512, 4, 4],\n",
    "    'features.12': [512, 4, 4],\n",
    "    'features.13': [512, 4, 4],\n",
    "    'features.14': [512, 4, 4],\n",
    "    'features.15': [512, 2, 2],\n",
    "    'features.16': [512, 2, 2],\n",
    "    'features.17': [512, 2, 2],\n",
    "    'features.18': [512, 2, 2],\n",
    "    'features.19': [512, 2, 2],\n",
    "    'features.20': [512, 1, 1],\n",
    "    'avgpool': [512, 7, 7],\n",
    "    'classifier.0': [4096],\n",
    "    'classifier.1': [4096],\n",
    "    'classifier.2': [4096],\n",
    "    'classifier.3': [4096],\n",
    "    'classifier.4': [4096],\n",
    "    'classifier.5': [4096],\n",
    "    'classifier.6': [10],\n",
    "}\n",
    "\n",
    "layer_list = VGG11_CIFAR10_FULL_LAYER_LIST\n",
    "layer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c6b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = torch.nn.Linear(28 * 28, 100)\n",
    "        self.layer2 = torch.nn.Linear(100, 10)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        return self.layer2(self.relu1(self.layer1(x)))\n",
    "    \n",
    "\n",
    "class Hooks(object):\n",
    "    def __init__(self):\n",
    "        self.dataframe = pandas.DataFrame(columns=[\"module_name\", \"tensor_type\", \"batch\", \"location\", \"value\", \"accuracy\", \"loss\"])\n",
    "        self.handles = []\n",
    "    \n",
    "    def make_neuron_output_function(self, module_name, location):\n",
    "        def save_neuron_output(module, args, output) -> None:\n",
    "            for b_idx, b in enumerate(output):\n",
    "                self.dataframe.loc[len(self.dataframe)] = [module_name, \"output\", b_idx, location, b[location].item(), None, None]\n",
    "        return save_neuron_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26f91cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss, optim,  epochs, train_dataloader, test_dataloader, model_checkpoint_path, load_checkpoint=True, save_checkpoint=True, device=torch.device(\"cpu\")):\n",
    "    if model_checkpoint_path.exists() and load_checkpoint:\n",
    "        model = torch.load(str(model_checkpoint_path), map_location=device)\n",
    "        return model\n",
    "    elif save_checkpoint:\n",
    "        model_checkpoint_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    for e in range(1, epochs + 1):\n",
    "        print(f\"Training {e}/{epochs}\")\n",
    "        train_loop(model, loss, optim, train_dataloader, device=device)\n",
    "        print(\"Testing\")\n",
    "        test_loop(model, loss, test_dataloader, device=device)\n",
    "    if save_checkpoint:\n",
    "        torch.save(model, str(model_checkpoint_path))\n",
    "    return model\n",
    "        \n",
    "        \n",
    "def train_loop(model, loss, optim, dataloader, device=torch.device(\"cpu\")):\n",
    "    model = model.train(True).to(device)\n",
    "    for b in dataloader:\n",
    "        x, y = b\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            y_hat = model(x)\n",
    "\n",
    "            l = loss(y_hat, y)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "    print(l)\n",
    "\n",
    "    \n",
    "def test_loop(model, loss, dataloader, device=torch.device(\"cpu\")):\n",
    "    model = model.train(False).to(device)\n",
    "    for b in dataloader:\n",
    "        x, y = b\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            y_hat = model(x)\n",
    "            l = loss(y_hat, y)\n",
    "    print(l)\n",
    "    \n",
    "def test_loop_save_accuracy_loss(model, loss, accuracy, dataloader, hooks, device=torch.device(\"cpu\"), dataframe_save_path=None):\n",
    "    model = model.train(False).to(device)\n",
    "    for idx, b in enumerate(dataloader):\n",
    "        x, y = b\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for b_idx, (bx, by) in enumerate(zip(x, y)):\n",
    "                bx = bx.unsqueeze(0)\n",
    "                by = by.unsqueeze(0)\n",
    "                by_hat = model(bx)\n",
    "                bl = loss(by_hat, by)\n",
    "                df = hooks.dataframe[(hooks.dataframe[\"accuracy\"] == None) & (hooks.dataframe[\"loss\"] == None)]\n",
    "                df[\"accuracy\"] = accuracy(by_hat, by).item()\n",
    "                df[\"loss\"] = bl.item()\n",
    "#             y_hat = model(x)\n",
    "#             l = loss(y_hat, y)\n",
    "#         for b_idx, (by, by_hat) in enumerate(zip(y, y_hat)):\n",
    "#             hooks.dataframe.loc[len(hooks.dataframe) - len(y_hat) + b_idx, \"accuracy\"] = accuracy(by_hat.unsqueeze(0), by.unsqueeze(0)).item()\n",
    "#             hooks.dataframe.loc[len(hooks.dataframe) - len(y_hat) + b_idx, \"loss\"] = loss(by_hat.unsqueeze(0), by.unsqueeze(0)).item()\n",
    "        if dataframe_save_path is not None:\n",
    "            dataframe_save_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "            hooks.dataframe.to_csv(dataframe_save_path, sep=\"|\")\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Batches done: {idx}\")\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af5ba03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.vgg11(num_classes=10, init_weights=True)\n",
    "# model = MLP()\n",
    "model_checkpoint_path = pathlib.Path(\"results/trained_vgg11_cifar10_epoch30.pt\")\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "# transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.7,), (0.7,)),])\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='/shared/ml/datasets/vision/CIFAR10', train=True, download=True, transform=transforms)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=\"/shared/ml/datasets/vision/CIFAR10\", train=False, download=True, transform=transforms)\n",
    "# train_dataset = torchvision.datasets.MNIST(root='/shared/ml/datasets/vision/MNIST/', train=True, download=True, transform=transforms)\n",
    "# test_dataset = torchvision.datasets.MNIST(root=\"/shared/ml/datasets/vision/MNIST/\", train=False, download=True, transform=transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a56e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(model, loss, optim, 30, train_dataloader, test_dataloader, model_checkpoint_path=model_checkpoint_path, load_checkpoint=True, save_checkpoint=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd5cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.train(False).to(torch.device(\"cpu\"))\n",
    "# attribution = captum.attr.LayerConductance(model, model.features[0])\n",
    "# it = iter(test_dataloader)\n",
    "# x, y = next(it)\n",
    "# attr1 = attribution.attribute(inputs=x, target=y)\n",
    "# print(x, y, attr1)\n",
    "# x, y = next(it)\n",
    "# attr2 = attribution.attribute(inputs=x, target=y)\n",
    "# print(x, y, attr2)\n",
    "# print(attr1 + attr2)\n",
    "# it = iter(test_dataloader)\n",
    "# x1, y1 = next(it)\n",
    "# x2, y2 = next(it)\n",
    "# attr = attribution.attribute(inputs=torch.cat([x1, x2]), target=torch.cat([y1, y2]))\n",
    "# print(x1, y1, x2, y2, attr)\n",
    "# print(attr1 + attr2 == attr)\n",
    "# attr.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55782ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_checkpoint_path = model_checkpoint_path.with_suffix(f\".attributions.pt\")\n",
    "attributions = get_attributions(\n",
    "    model,\n",
    "    test_dataloader,\n",
    "    list(layer_list.keys()),\n",
    "    attributions_checkpoint_path=attributions_checkpoint_path,\n",
    "    save_checkpoint=True,\n",
    "    load_checkpoint=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04c51831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('results/trained_vgg11_cifar10_epoch30.2023_04_12__10_30_54_+0200.csv')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooks = Hooks()\n",
    "# hooks.handles.append(model.layer1.register_forward_hook(hooks.make_neuron_output_function(\"layer1\", 0)))\n",
    "# for layer_name, layer_size in layer_list.items():\n",
    "#     for layer_position in itertools.product(*(range(l_s) for l_s in layer_size)):\n",
    "#         module = get_layer_from_full_name(model, layer_name, separator=\".\", main_model_is_in_the_layer_name=False)\n",
    "#         hooks.handles.append(module.register_forward_hook(hooks.make_neuron_output_function(layer_name, tuple(layer_position))))\n",
    "for layer_name, layer_attributions_and_deltas in attributions.items():\n",
    "    layer_attributions_cat = torch.cat(tuple(l_attr for l_attr, _ in layer_attributions_and_deltas), dim=0, )\n",
    "    summed_layer_attributions = torch.sum(layer_attributions_cat, (0, ), )\n",
    "    target_neuron_location = numpy.unravel_index(\n",
    "        torch.argmax(\n",
    "            abs(\n",
    "                summed_layer_attributions, \n",
    "            ), \n",
    "        ).item(),\n",
    "        summed_layer_attributions.size(),\n",
    "        order=\"C\",\n",
    "    )\n",
    "    module = get_layer_from_full_name(model, layer_name, separator=\".\", main_model_is_in_the_layer_name=False)\n",
    "    hooks.handles.append(module.register_forward_hook(hooks.make_neuron_output_function(layer_name, tuple(target_neuron_location))))\n",
    "dataframe_path = model_checkpoint_path.with_suffix(f\".{time.strftime(TIME_FORMAT)}.csv\")\n",
    "dataframe_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e987d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches done: 0\n",
      "Batches done: 10\n",
      "Batches done: 20\n",
      "Batches done: 30\n",
      "Batches done: 40\n",
      "Batches done: 50\n",
      "Batches done: 60\n",
      "Batches done: 70\n",
      "Batches done: 80\n",
      "Batches done: 90\n",
      "Batches done: 100\n"
     ]
    }
   ],
   "source": [
    "test_loop_save_accuracy_loss(model, loss, accuracy, test_dataloader, hooks, device=device, dataframe_save_path=dataframe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly.express.scatter(\n",
    "    hooks.dataframe[hooks.dataframe[\"module_name\"] == \"features.0\"],\n",
    "    y=\"value\",\n",
    "    x=\"loss\",\n",
    "    marginal_x=\"histogram\",\n",
    "    marginal_y=\"histogram\",\n",
    "    color=\"accuracy\",\n",
    "    # CIFAR10 with VGG11 30 epochs\n",
    "    range_x=[-0.1, 160],\n",
    "    range_y=[-1, 1],\n",
    "    # MNIST with MLP 30 epochs\n",
    "#     range_x=[-0.1, 20],\n",
    "#     range_y=[-20, 20],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
