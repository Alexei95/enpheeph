# enpheeph - Neural Fault Injection Framework
# Copyright (C) 2020-2023 Alessio "Alexei95" Colucci
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944b3e86",
   "metadata": {},
   "source": [
    "# Use the enpheeph-dev mamba environment\n",
    "The old one is enpheeph-dev-old-lightning-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437a3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import time\n",
    "\n",
    "import captum\n",
    "import lightning\n",
    "import numpy\n",
    "import pandas\n",
    "import plotly\n",
    "import plotly.express\n",
    "import torch\n",
    "import torch.optim\n",
    "import torchinfo\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649f0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(lightning.LightningModule):\n",
    "    @property\n",
    "    def LAYER_LIST(self):\n",
    "        return {\n",
    "            \"vgg11\": {\n",
    "                'model.features.0': [64, 32, 32],\n",
    "                'model.features.1': [64, 32, 32],\n",
    "                'model.features.2': [64, 16, 16],\n",
    "                'model.features.3': [128, 16, 16],\n",
    "                'model.features.4': [128, 16, 16],\n",
    "                'model.features.5': [128, 8, 8],\n",
    "                'model.features.6': [256, 8, 8],\n",
    "                'model.features.7': [256, 8, 8],\n",
    "                'model.features.8': [256, 8, 8],\n",
    "                'model.features.9': [256, 8, 8],\n",
    "                'model.features.10': [256, 4, 4],\n",
    "                'model.features.11': [512, 4, 4],\n",
    "                'model.features.12': [512, 4, 4],\n",
    "                'model.features.13': [512, 4, 4],\n",
    "                'model.features.14': [512, 4, 4],\n",
    "                'model.features.15': [512, 2, 2],\n",
    "                'model.features.16': [512, 2, 2],\n",
    "                'model.features.17': [512, 2, 2],\n",
    "                'model.features.18': [512, 2, 2],\n",
    "                'model.features.19': [512, 2, 2],\n",
    "                'model.features.20': [512, 1, 1],\n",
    "                'model.avgpool': [512, 7, 7],\n",
    "                'model.classifier.0': [4096],\n",
    "                'model.classifier.1': [4096],\n",
    "                'model.classifier.2': [4096],\n",
    "                'model.classifier.3': [4096],\n",
    "                'model.classifier.4': [4096],\n",
    "                'model.classifier.5': [4096],\n",
    "                'model.classifier.6': [self.num_classes],\n",
    "            },\n",
    "            \"resnet18\": {\n",
    "                'model.conv1': [64, 16, 16],\n",
    "                'model.bn1': [64, 16, 16],\n",
    "                'model.relu': [64, 16, 16],\n",
    "                'model.maxpool': [64, 8, 8],\n",
    "                'model.layer1.0.conv1': [64, 8, 8],\n",
    "                'model.layer1.0.bn1': [64, 8, 8],\n",
    "                'model.layer1.0.relu': [64, 8, 8],\n",
    "                'model.layer1.0.conv2': [64, 8, 8],\n",
    "                'model.layer1.0.bn2': [64, 8, 8],\n",
    "                'model.layer1.1.conv1': [64, 8, 8],\n",
    "                'model.layer1.1.bn1': [64, 8, 8],\n",
    "                'model.layer1.1.relu': [64, 8, 8],\n",
    "                'model.layer1.1.conv2': [64, 8, 8],\n",
    "                'model.layer1.1.bn2': [64, 8, 8],\n",
    "                'model.layer2.0.conv1': [128, 4, 4],\n",
    "                'model.layer2.0.bn1': [128, 4, 4],\n",
    "                'model.layer2.0.relu': [128, 4, 4],\n",
    "                'model.layer2.0.conv2': [128, 4, 4],\n",
    "                'model.layer2.0.bn2': [128, 4, 4],\n",
    "                'model.layer2.0.downsample.0': [128, 4, 4],\n",
    "                'model.layer2.0.downsample.1': [128, 4, 4],\n",
    "                'model.layer2.1.conv1': [128, 4, 4],\n",
    "                'model.layer2.1.bn1': [128, 4, 4],\n",
    "                'model.layer2.1.relu': [128, 4, 4],\n",
    "                'model.layer2.1.conv2': [128, 4, 4],\n",
    "                'model.layer2.1.bn2': [128, 4, 4],\n",
    "                'model.layer3.0.conv1': [256, 2, 2],\n",
    "                'model.layer3.0.bn1': [256, 2, 2],\n",
    "                'model.layer3.0.relu': [256, 2, 2],\n",
    "                'model.layer3.0.conv2': [256, 2, 2],\n",
    "                'model.layer3.0.bn2': [256, 2, 2],\n",
    "                'model.layer3.0.downsample.0': [256, 2, 2],\n",
    "                'model.layer3.0.downsample.1': [256, 2, 2],\n",
    "                'model.layer3.1.conv1': [256, 2, 2],\n",
    "                'model.layer3.1.bn1': [256, 2, 2],\n",
    "                'model.layer3.1.relu': [256, 2, 2],\n",
    "                'model.layer3.1.conv2': [256, 2, 2],\n",
    "                'model.layer3.1.bn2': [256, 2, 2],\n",
    "                'model.layer4.0.conv1': [512, 1, 1],\n",
    "                'model.layer4.0.bn1': [512, 1, 1],\n",
    "                'model.layer4.0.relu': [512, 1, 1],\n",
    "                'model.layer4.0.conv2': [512, 1, 1],\n",
    "                'model.layer4.0.bn2': [512, 1, 1],\n",
    "                'model.layer4.0.downsample.0': [512, 1, 1],\n",
    "                'model.layer4.0.downsample.1': [512, 1, 1],\n",
    "                'model.layer4.1.conv1': [512, 1, 1],\n",
    "                'model.layer4.1.bn1': [512, 1, 1],\n",
    "                'model.layer4.1.relu': [512, 1, 1],\n",
    "                'model.layer4.1.conv2': [512, 1, 1],\n",
    "                'model.layer4.1.bn2': [512, 1, 1],\n",
    "                'model.avgpool': [512, 1, 1],\n",
    "                'model.fc': [10],\n",
    "            },\n",
    "        }\n",
    "    \n",
    "    def __init__(self, model_name, num_classes, accuracy_fn, loss_fn, dataframe_path, optimizer_class, learning_rate, dataset_name=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model_name = model_name.lower()\n",
    "        self.num_classes = num_classes\n",
    "        self.accuracy = accuracy_fn\n",
    "        self.loss = loss_fn\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.dataframe_path = pathlib.Path(dataframe_path)\n",
    "        \n",
    "        self.setup_model(model_name=self.model_name, num_classes=self.num_classes)\n",
    "        \n",
    "        self.handles = []\n",
    "        \n",
    "        self.reset_dataframe()\n",
    "        self.init_model()\n",
    "        \n",
    "    def setup_model(self, model_name, num_classes):\n",
    "        if model_name == \"vgg11\":\n",
    "            self.model = torchvision.models.vgg11(num_classes=num_classes)\n",
    "        elif model_name == \"resnet18\":\n",
    "            self.model = torchvision.models.resnet18(num_classes=num_classes)\n",
    "        elif model_name == \"mlp\":\n",
    "            self.model = torch.nn.Sequential(\n",
    "                torch.nn.Flatten(),\n",
    "                torch.nn.Linear(28 * 28, 100),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(100, num_classes),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"unknown model\")\n",
    "        \n",
    "    def init_model(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (torch.nn.BatchNorm2d, torch.nn.GroupNorm)):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def reset_dataframe(self):\n",
    "        self.dataframe = pandas.DataFrame(columns=[\"module_name\", \"tensor_type\", \"batch_index\", \"element_in_batch_index\", \"location\", \"neuron_attribution_sorting\", \"value\", \"accuracy\", \"loss\"])\n",
    "        \n",
    "    @staticmethod\n",
    "    def join_saved_dataframe(dataframe, dataframe_path: os.PathLike):\n",
    "        dataframe_path = pathlib.Path(dataframe_path)\n",
    "        if not dataframe_path.exists():\n",
    "            dataframe_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            dataframe.to_csv(dataframe_path, sep=\"|\")\n",
    "        else:\n",
    "            df = pandas.read_csv(dataframe_path, sep=\"|\", index_col=[0], header=[0])\n",
    "            new_df = pandas.concat([df, dataframe], axis=0)\n",
    "            new_df.reset_index(drop=True, inplace=True)\n",
    "            new_df.to_csv(dataframe_path, sep=\"|\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer_class(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def make_neuron_output_function(self, module_name, location, neuron_attribution_sorting):\n",
    "        def save_neuron_output(module, args, output) -> None:\n",
    "            for b_idx, b in enumerate(output):\n",
    "                self.dataframe.loc[len(self.dataframe)] = [module_name, \"output\", None, b_idx, location, neuron_attribution_sorting, b[location].item(), None, None]\n",
    "        return save_neuron_output\n",
    "    \n",
    "    def add_hooks(self, attributions, topk=1, bottomk=1):\n",
    "        for layer_name, layer_attributions_and_deltas in attributions.items():\n",
    "            layer_attributions_cat = torch.cat(tuple(l_attr for l_attr, _ in layer_attributions_and_deltas), dim=0, )\n",
    "            summed_layer_attributions = torch.sum(layer_attributions_cat, (0, ), )\n",
    "            topk_values, topk_indices = torch.topk(\n",
    "                abs(\n",
    "                    summed_layer_attributions.flatten(), \n",
    "                ),\n",
    "                k=topk,\n",
    "                largest=True,\n",
    "                sorted=True,\n",
    "            )\n",
    "            bottomk_values, bottomk_indices = torch.topk(\n",
    "                abs(\n",
    "                    summed_layer_attributions.flatten(), \n",
    "                ),\n",
    "                k=bottomk,\n",
    "                largest=False,\n",
    "                sorted=True,\n",
    "            )\n",
    "            indices = [{\"neuron_attribution_sorting\": f\"top{i}\", \"index\": idx} for i, idx in enumerate(topk_indices)] + [{\"neuron_attribution_sorting\": f\"bottom{i}\", \"index\": idx} for i, idx in enumerate(bottomk_indices)]\n",
    "            for index in indices:\n",
    "                target_neuron_location = numpy.unravel_index(\n",
    "                    index[\"index\"],\n",
    "                    summed_layer_attributions.size(),\n",
    "                    order=\"C\",\n",
    "                )\n",
    "                module = self.get_layer_from_full_name(self, layer_name, separator=\".\", main_model_is_in_the_layer_name=False)\n",
    "                self.handles.append(module.register_forward_hook(self.make_neuron_output_function(layer_name, tuple(target_neuron_location), neuron_attribution_sorting=index[\"neuron_attribution_sorting\"])))\n",
    "         \n",
    "    @staticmethod\n",
    "    def get_full_layer_name_from_summary(layer_summary, skip_main_model=True):\n",
    "        parent_info = layer_summary.parent_info\n",
    "        layer_full_name = layer_summary.var_name\n",
    "        while parent_info is not None and (not skip_main_model or skip_main_model and parent_info.parent_info is not None):\n",
    "            layer_full_name = f\"{parent_info.var_name}.{layer_full_name}\"\n",
    "            parent_info = parent_info.parent_info\n",
    "        return layer_full_name\n",
    "\n",
    "    @staticmethod\n",
    "    def get_layer_from_full_name(model, layer_name, separator=\".\", main_model_is_in_the_layer_name=False):\n",
    "        module = model\n",
    "        if main_model_is_in_the_layer_name:\n",
    "            layer_name = separator.join(layer_name.split(separator)[1:])\n",
    "        for l_n in layer_name.split(separator):\n",
    "            module = getattr(module, l_n)\n",
    "        return module\n",
    "\n",
    "    def get_attributions(self, dataloader, layer_name_list, attributions_checkpoint_path, attribution=captum.attr.LayerConductance, save_checkpoint=True, load_checkpoint=True):\n",
    "        if attributions_checkpoint_path.exists() and load_checkpoint:\n",
    "            attributions = torch.load(str(attributions_checkpoint_path))\n",
    "            return attributions\n",
    "        elif save_checkpoint:\n",
    "            attributions_checkpoint_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        model = self.train(False).to(torch.device(\"cpu\"))\n",
    "\n",
    "        attributions = {}\n",
    "        for layer_name in layer_name_list:\n",
    "            print(layer_name)\n",
    "            layer_attributions = []\n",
    "            attr_instance = attribution(model, model.get_layer_from_full_name(model, layer_name))\n",
    "            for idx, b in enumerate(dataloader):\n",
    "                x, y = b\n",
    "                attr, delta = attr_instance.attribute(\n",
    "                    inputs=x.to(torch.device(\"cpu\")),\n",
    "                    target=y.to(torch.device(\"cpu\")),\n",
    "                    return_convergence_delta=True,\n",
    "                )\n",
    "                layer_attributions.append(\n",
    "                    [attr.detach(), delta.detach(), ],\n",
    "                )\n",
    "                if idx % 10 == 0:\n",
    "                    print(f\"Batches done: {idx}\")\n",
    "            attributions[layer_name] = layer_attributions\n",
    "            if save_checkpoint:\n",
    "                torch.save(attributions, str(attributions_checkpoint_path))\n",
    "                \n",
    "        if save_checkpoint:\n",
    "            torch.save(attributions, str(attributions_checkpoint_path))\n",
    "\n",
    "        return attributions\n",
    "    \n",
    "    def inference_step(self, batch, only_x=False):\n",
    "        if only_x:\n",
    "            x = batch\n",
    "        else:\n",
    "            x, y = batch\n",
    "        y_hat = self(x)\n",
    "        if only_x:\n",
    "            d = {\"loss\": None, \"accuracy\": None, \"predictions\": y_hat}\n",
    "        else:\n",
    "            d = {\"loss\": self.loss(y_hat, y), \"accuracy\": self.accuracy(y_hat, y), \"predictions\": y_hat}\n",
    "        return d\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        metrics = self.inference_step(batch)\n",
    "        self.log_dict({\"train_loss\": metrics[\"loss\"], \"train_accuracy\": metrics[\"accuracy\"]}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return metrics[\"loss\"]\n",
    "    \n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        metrics = self.inference_step(batch)\n",
    "        self.log_dict({\"test_loss\": metrics[\"loss\"], \"test_accuracy\": metrics[\"accuracy\"]}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return metrics[\"predictions\"]\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        metrics = self.inference_step(batch)\n",
    "        self.log_dict({\"val_loss\": metrics[\"loss\"], \"val_accuracy\": metrics[\"accuracy\"]}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return metrics[\"predictions\"]\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        metrics = self.inference_step(batch, only_x=True)\n",
    "        # self.log({\"val_loss\": metrics[\"loss\"], \"val_accuracy\": metrics[\"accuracy\"]}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return metrics[\"predictions\"]\n",
    "        \n",
    "    def on_test_batch_end(self, outputs, batch, batch_idx, dataloader_idx=0):\n",
    "        # super().on_test_batch_end(outputs, batch, batch_idx, dataloader_idx)\n",
    "        _, y = batch\n",
    "        row_selector = self.dataframe[\"accuracy\"].isnull() & self.dataframe[\"loss\"].isnull()\n",
    "        self.dataframe.loc[row_selector, \"batch_index\"] = batch_idx\n",
    "        assert len(self.dataframe.loc[row_selector]) / len(self.handles) == y.size()[0] == outputs.size()[0]\n",
    "        for bindex, (by_hat, by) in enumerate(zip(outputs, y)):\n",
    "            by_hat = by_hat.unsqueeze(0)\n",
    "            by = by.unsqueeze(0)\n",
    "            extra_row_selector = row_selector & (self.dataframe[\"element_in_batch_index\"] == bindex)\n",
    "            self.dataframe.loc[extra_row_selector, \"loss\"] = self.loss(by_hat, by).item()\n",
    "            self.dataframe.loc[extra_row_selector, \"accuracy\"] = self.accuracy(by_hat, by).item()\n",
    "        self.dataframe_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if batch_idx % 10 == 0:\n",
    "            self.join_saved_dataframe(self.dataframe, self.dataframe_path)\n",
    "            self.reset_dataframe()\n",
    "        # print(self.dataframe)\n",
    "        \n",
    "    def on_test_end(self):\n",
    "        self.join_saved_dataframe(self.dataframe, self.dataframe_path)\n",
    "        \n",
    "\n",
    "class DataModule(lightning.LightningDataModule):\n",
    "    MNIST_DEFAULT_TRANSFORM = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((28, 28)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            (0.1307,),\n",
    "            (0.3081,),\n",
    "        ),\n",
    "    ])\n",
    "    CIFAR10_DEFAULT_TRANSFORM = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((32, 32)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "            std=[x / 255.0 for x in [63.0, 62.1, 66.7]],\n",
    "        ),\n",
    "    ])\n",
    "    GTSRB_DEFAULT_TRANSFORM = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((48, 48)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            (0.3337, 0.3064, 0.3171),\n",
    "            (0.2672, 0.2564, 0.2629),\n",
    "        ),\n",
    "    ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def gtsrb_wrapper(data_dir, train: bool, transform=None, download: bool = True):\n",
    "        if train is True:\n",
    "            split = \"train\"\n",
    "        elif train is False:\n",
    "            split = \"test\"\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        return torchvision.datasets.GTSRB(\n",
    "            str(data_dir),\n",
    "            split=split,\n",
    "            download=download,\n",
    "            transform=transform,\n",
    "        )\n",
    "    \n",
    "    def __init__(self, dataset_name, data_dir: str = \"/shared/ml/datasets/vision/\", train_transform=None, test_transform=None, batch_size=64, num_workers=32, train_val_split=0.8, seed=42, dataset_class=None):\n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name.lower()\n",
    "        self.dataset_class = dataset_class\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.train_val_split = train_val_split\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        self.train_transform = train_transform\n",
    "        self.test_transform = test_transform\n",
    "        \n",
    "        self.num_classes = None\n",
    "        \n",
    "        self.setup_dataset()\n",
    "        \n",
    "    def setup_dataset(self):\n",
    "        if self.dataset_class is None:\n",
    "            if self.dataset_name == \"cifar10\":\n",
    "                self.dataset_class = torchvision.datasets.CIFAR10\n",
    "            elif self.dataset_name == \"mnist\":\n",
    "                self.dataset_class = torchvision.datasets.MNIST\n",
    "            elif self.dataset_name == \"gtsrb\":\n",
    "                self.dataset_class = self.__class__.gtsrb_wrapper\n",
    "        \n",
    "        if self.dataset_class == self.__class__.gtsrb_wrapper:\n",
    "            if self.train_transform is None:\n",
    "                self.train_transform = torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.Resize((48, 48)),\n",
    "                    torchvision.transforms.RandomCrop(48),\n",
    "                    torchvision.transforms.RandomHorizontalFlip(),\n",
    "                    torchvision.transforms.RandomVerticalFlip(),\n",
    "                    self.GTSRB_DEFAULT_TRANSFORM,\n",
    "                ])\n",
    "            if self.test_transform is None:\n",
    "                self.test_transform = self.GTSRB_DEFAULT_TRANSFORM\n",
    "            self.num_classes = 43\n",
    "        elif self.dataset_class == torchvision.datasets.MNIST or issubclass(self.dataset_class, torchvision.datasets.MNIST):\n",
    "            if self.train_transform is None:\n",
    "                self.train_transform = torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.Resize((28, 28)),\n",
    "                    torchvision.transforms.RandomCrop(28, padding=4),\n",
    "                    self.MNIST_DEFAULT_TRANSFORM,\n",
    "                ])\n",
    "            if self.test_transform is None:\n",
    "                self.test_transform = self.MNIST_DEFAULT_TRANSFORM\n",
    "            self.num_classes = 10\n",
    "        elif self.dataset_class == torchvision.datasets.CIFAR10 or issubclass(self.dataset_class, torchvision.datasets.CIFAR10):\n",
    "            if self.train_transform is None:\n",
    "                self.train_transform = torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.Resize((32, 32)),\n",
    "                    torchvision.transforms.RandomCrop(32, padding=4),\n",
    "                    torchvision.transforms.RandomHorizontalFlip(),\n",
    "                    self.CIFAR10_DEFAULT_TRANSFORM,\n",
    "                ])\n",
    "            if self.test_transform is None:\n",
    "                self.test_transform = self.CIFAR10_DEFAULT_TRANSFORM\n",
    "            self.num_classes = 10\n",
    "        else:\n",
    "            raise ValueError(\"unknown dataset\")\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        self.dataset_class(self.data_dir, train=True, download=True)\n",
    "        self.dataset_class(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\":\n",
    "            dataset_train_transform = self.dataset_class(self.data_dir, train=True, transform=self.train_transform)\n",
    "            n_train_elements = math.floor(len(dataset_train_transform) * self.train_val_split)\n",
    "            self.dataset_train, _ = torch.utils.data.random_split(\n",
    "                dataset_train_transform,\n",
    "                [n_train_elements, len(dataset_train_transform) - n_train_elements],\n",
    "                generator=torch.Generator().manual_seed(self.seed),\n",
    "            )\n",
    "            dataset_test_transform = self.dataset_class(self.data_dir, train=True, transform=self.test_transform)\n",
    "            _, self.dataset_val = torch.utils.data.random_split(\n",
    "                dataset_test_transform,\n",
    "                [n_train_elements, len(dataset_train_transform) - n_train_elements],\n",
    "                generator=torch.Generator().manual_seed(self.seed),\n",
    "            )\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\":\n",
    "            self.dataset_test = self.dataset_class(self.data_dir, train=False, transform=self.test_transform)\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            self.dataset_predict = self.dataset_class(self.data_dir, train=False, transform=self.test_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.dataset_train, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.dataset_val, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.dataset_test, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.dataset_predict, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0213829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIME_FORMAT = \"%Y_%m_%d__%H_%M_%S_%z\"\n",
    "\n",
    "time_string = time.strftime(TIME_FORMAT)\n",
    "\n",
    "model_name = \"resnet18\"\n",
    "dataset_name = \"GTSRB\"\n",
    "pruned = False\n",
    "sparse = False\n",
    "\n",
    "base_path = pathlib.Path(f\"./results/trained_{model_name}_{dataset_name}_{'pruned' if pruned else 'original'}_{'sparse' if sparse else 'dense'}_earlystopping_lightning\")\n",
    "model_checkpoint_path = base_path.with_suffix(f\".{time_string}.pt\")\n",
    "attributions_checkpoint_path = base_path.with_suffix(f\".{time_string}.attributions.pt\")\n",
    "dataframe_path = base_path.with_suffix(f\".{time_string}.csv\")\n",
    "\n",
    "learning_rate_finder = False\n",
    "# seed = 7  # vgg11 cifar10\n",
    "seed = 7  # resnet18 gtsrb\n",
    "\n",
    "lightning.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a56e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/acolucci/micromamba/envs/enpheeph-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "/home/acolucci/micromamba/envs/enpheeph-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:197: UserWarning: Attribute 'accuracy_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['accuracy_fn'])`.\n",
      "  rank_zero_warn(\n",
      "/home/acolucci/micromamba/envs/enpheeph-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:197: UserWarning: Attribute 'loss_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_fn'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    lightning.pytorch.callbacks.EarlyStopping(\n",
    "        \"val_loss\",\n",
    "        min_delta=0.001,\n",
    "        patience=5,\n",
    "        verbose=True,\n",
    "        mode='min',\n",
    "        strict=True,\n",
    "        check_finite=True,\n",
    "        stopping_threshold=None,\n",
    "        divergence_threshold=None,\n",
    "        check_on_train_epoch_end=None,\n",
    "        log_rank_zero_only=False,\n",
    "    ),\n",
    "    lightning.pytorch.callbacks.ModelCheckpoint(\n",
    "        dirpath=None,\n",
    "        filename=None,\n",
    "        monitor=None,\n",
    "        verbose=False,\n",
    "        save_last=None,\n",
    "        # to disable model saving\n",
    "        save_top_k=0,\n",
    "        save_weights_only=False,\n",
    "        mode='min',\n",
    "        auto_insert_metric_name=True,\n",
    "        every_n_train_steps=None,\n",
    "        train_time_interval=None,\n",
    "        every_n_epochs=None,\n",
    "        # to save at the end of validation, so that if we use pruning\n",
    "        # it can run at the end of training\n",
    "        save_on_train_epoch_end=False,\n",
    "    ),\n",
    "    lightning.pytorch.callbacks.RichProgressBar(\n",
    "        refresh_rate=10,\n",
    "    ),\n",
    "    lightning.pytorch.callbacks.StochasticWeightAveraging(\n",
    "        swa_lrs=1e-2,\n",
    "    ),\n",
    "]\n",
    "if pruned:\n",
    "    callbacks.append(\n",
    "        pytorch_lightning.callbacks.ModelPruning(\n",
    "            amount=compute_pruning_amount,\n",
    "            apply_pruning=True,\n",
    "            make_pruning_permanent=True,\n",
    "            parameter_names=(\"weight\", \"bias\"),\n",
    "            parameters_to_prune=None,\n",
    "            prune_on_train_epoch_end=True,\n",
    "            pruning_dim=None,\n",
    "            # pruning_fn=\"random_unstructured\",\n",
    "            pruning_fn=\"l1_unstructured\",\n",
    "            pruning_norm=None,\n",
    "            resample_parameters=True,\n",
    "            use_global_unstructured=True,\n",
    "            use_lottery_ticket_hypothesis=True,\n",
    "            verbose=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "trainer = lightning.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[2],\n",
    "    # setting max_epochs make it not work, it stops with max_epochs=0, also fast_dev_run=True breaks it\n",
    "    # fast_dev_run=True,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "datamodule = DataModule(\n",
    "    dataset_name=dataset_name,\n",
    "    data_dir=f\"/shared/ml/datasets/vision/{dataset_name}\",\n",
    "    train_transform=None,\n",
    "    test_transform=None,\n",
    "    batch_size=64,\n",
    "    train_val_split=0.8,\n",
    "    seed=seed,\n",
    ")\n",
    "model = Model(\n",
    "    model_name=model_name,\n",
    "    num_classes=datamodule.num_classes,\n",
    "    accuracy_fn=torchmetrics.Accuracy(\n",
    "        task=\"multiclass\",\n",
    "        num_classes=datamodule.num_classes,\n",
    "    ),\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "    dataframe_path=dataframe_path,\n",
    "    optimizer_class=torch.optim.AdamW,\n",
    "    learning_rate=2e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c9d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acolucci/micromamba/envs/enpheeph-dev/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name     </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type               </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ accuracy │ MulticlassAccuracy │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ loss     │ CrossEntropyLoss   │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ model    │ ResNet             │ 11.2 M │\n",
       "└───┴──────────┴────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType              \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ accuracy │ MulticlassAccuracy │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ loss     │ CrossEntropyLoss   │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ model    │ ResNet             │ 11.2 M │\n",
       "└───┴──────────┴────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 11.2 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 11.2 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 44                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 11.2 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 11.2 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 44                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b661debdb9dd49e1bbe2b9ad221cc92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.789\n"
     ]
    }
   ],
   "source": [
    "if learning_rate_finder:\n",
    "    tuner = lightning.pytorch.tuner.Tuner(trainer)\n",
    "    tuner.lr_find(model, datamodule=datamodule)\n",
    "    print(model.learning_rate)\n",
    "    raise Exception()\n",
    "\n",
    "if model_checkpoint_path.exists():\n",
    "    model.__class__.load_from_checkpoint(str(model_checkpoint_path))\n",
    "else:\n",
    "    model_checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    trainer.fit(model, datamodule)\n",
    "    trainer.save_checkpoint(str(model_checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55782ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.prepare_data()\n",
    "datamodule.setup(stage=\"test\")\n",
    "\n",
    "attributions = model.__class__.get_attributions(\n",
    "    model,\n",
    "    datamodule.test_dataloader(),\n",
    "    list(model.LAYER_LIST[model.model_name].keys()),\n",
    "    attributions_checkpoint_path=attributions_checkpoint_path,\n",
    "    save_checkpoint=True,\n",
    "    load_checkpoint=True,\n",
    ")\n",
    "\n",
    "datamodule.teardown(stage=\"test\")\n",
    "\n",
    "model.add_hooks(attributions, topk=5, bottomk=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e987d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, datamodule, ckpt_path=str(model_checkpoint_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
